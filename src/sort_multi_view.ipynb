{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "from PyLQR.sim import KDLRobot\n",
    "from PyLQR.system import PosOrnPlannerSys, PosOrnKeypoint\n",
    "from PyLQR.solver import BatchILQRCP, BatchILQR, ILQRRecursive\n",
    "from PyLQR.utils import primitives, PythonCallbackMessage\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D # <--- This is important for 3d plotting \n",
    "import cv2\n",
    "import matplotlib.cm as cm\n",
    "from tqdm import tqdm\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "from scipy.spatial.transform import Rotation \n",
    "\n",
    "from contact_grasp.srv import *\n",
    "from cv_bridge import CvBridge, CvBridgeError\n",
    "\n",
    "sys.path.append(\"/home/vdrame/catkin_ws/src/py_panda/PyPanda\")\n",
    "from PyPanda import Robot\n",
    "import rospy\n",
    "from PyPanda import Utils\n",
    "\n",
    "from utils.camera_utils import RealCamera, RealCameraROS\n",
    "from utils.transform_utils import *\n",
    "from utils.iLQR_wrapper import iLQR\n",
    "from utils.visualisation_utils import depth2pc\n",
    "\n",
    "import argparse\n",
    "from scipy.spatial.transform import Rotation\n",
    "\n",
    "import time\n",
    "\n",
    "from sensor_msgs.msg import Image, PointCloud2\n",
    "\n",
    "import rospy\n",
    "from contact_grasp.srv import contactGraspnetPointcloud2, contactGraspnetPointcloud2Response\n",
    "# from contact_grasp.transform_utils import pose_inv\n",
    "\n",
    "import json\n",
    "from cv_bridge import CvBridge, CvBridgeError\n",
    "from sensor_msgs import point_cloud2\n",
    "from sensor_msgs.msg import PointField, CameraInfo\n",
    "from std_msgs.msg import Header\n",
    "\n",
    "import open3d as o3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_action(rbt, actions, control_freq, eef_pos=None, eef_quat=None, segmentation_type=None, show_agentview=False, object_range=[5,8]):\n",
    "    success = False\n",
    "    rate = rospy.Rate(int(control_freq))\n",
    "\n",
    "    for idx, action in tqdm(enumerate(actions)):\n",
    "        rbt.active_controller.send_command(action)\n",
    "        rate.sleep()\n",
    "        #env.sim.step()\n",
    "        if eef_pos is not None:\n",
    "            eef_pos.append(rbt.model.ee_pos_rel())\n",
    "        if eef_quat is not None:\n",
    "            eef_quat.append(rbt.model.ee_orn_rel())\n",
    "    success = True\n",
    "    return success, idx, eef_pos, eef_quat\n",
    "\n",
    "def generate_grasps_client(pc2, bgr8):\n",
    "    rospy.wait_for_service('generate_grasps_pc')\n",
    "    try:\n",
    "        print(\"calling service\")\n",
    "        generate_grasps = rospy.ServiceProxy('generate_grasps_pc', contactGraspnetPointcloud2)\n",
    "        resp1 = generate_grasps(pc2, bgr8)\n",
    "        return resp1.quat, resp1.pos, resp1.opening.data, resp1.score.data, resp1.detected.data, resp1.detected_with_collision.data\n",
    "    except rospy.ServiceException as e:\n",
    "        print(\"Service call failed: %s\"%e)\n",
    "\n",
    "def format_pointcloud_msg(points, colors):\n",
    "    points = np.hstack((points, colors)).astype(dtype=object)\n",
    "    points[:,3:] = points[:,3:].astype(np.uint8)\n",
    "    fields = [PointField('x', 0, PointField.FLOAT32, 1),\n",
    "          PointField('y', 4, PointField.FLOAT32, 1),\n",
    "          PointField('z', 8, PointField.FLOAT32, 1),\n",
    "          # PointField('rgb', 12, PointField.UINT32, 1),\n",
    "          PointField('r', 12, PointField.UINT8, 1),\n",
    "          PointField('g', 13, PointField.UINT8, 1),\n",
    "          PointField('b', 14, PointField.UINT8, 1),\n",
    "          ]\n",
    "    \n",
    "    header = Header()\n",
    "    header.frame_id = 'camera_link'\n",
    "    pc2 = point_cloud2.create_cloud(header, fields, points)\n",
    "    return pc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_camera_pose(rbt):\n",
    "    \"\"\" Get camera pose in robot base frame\n",
    "    \"\"\"\n",
    "    ee_pose = np.eye(4)\n",
    "    ee_pose[:3,:3] = quat2mat(convert_quat(rbt.model.ee_orn_rel(), to=\"xyzw\")) #xyzw\n",
    "    ee_pose[:3,3] = rbt.model.ee_pos_rel() \n",
    "\n",
    "    ee2hand = np.eye(4)\n",
    "    ee2hand[2,3] = -0.1034\n",
    "\n",
    "    with open('config/camera_calibration.json') as json_file:\n",
    "        camera_calibration = json.load(json_file)\n",
    "\n",
    "    camera_type = \"L515\" #D415 or L515\n",
    "\n",
    "    hand2camera_pos = np.array(camera_calibration[camera_type][\"pos\"])\n",
    "    hand2camera_quat = camera_calibration[camera_type][\"quat_xyzw\"] #xyzw\n",
    "\n",
    "    # TODO todelete\n",
    "    # #D415\n",
    "    # # hand2camera_pos = np.array([0.0488546636437146,-0.03384417860749521,0.0512776975002817]) \n",
    "    # # hand2camera_quat = [0.012961267509189803,-0.0012768531849757236,0.7052247395136084,0.708864191484139] #xyzw \n",
    "\n",
    "    # #L515\n",
    "    # hand2camera_pos = np.array([0.08329189218278059, 0.0014213145240625528, 0.0504764049956106]) \n",
    "    # hand2camera_quat = [0.01521805627198811, 0.00623363612254646, 0.712108725756912, 0.7018765669580811] #xyzw \n",
    "\n",
    "    hand2camera_mat = Rotation.from_quat(hand2camera_quat).as_matrix()\n",
    "\n",
    "    hand2camera = np.eye(4)\n",
    "    hand2camera[:3,:3] = hand2camera_mat\n",
    "    hand2camera[:3,3] = hand2camera_pos\n",
    "\n",
    "    current_pose = ee_pose @ ee2hand @ hand2camera\n",
    "\n",
    "    return current_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera topic found\n"
     ]
    }
   ],
   "source": [
    "bridge = CvBridge()\n",
    "rospy.init_node(\"python_node\",anonymous=True)\n",
    "\n",
    "dispose_pos = np.array([0.1, 0.66, 0.1])\n",
    "dispose_orn_wxyz = np.array([0, 1, 0.35, 0])\n",
    "\n",
    "# Load robot\n",
    "rbt = Robot(\"panda\", use_gripper=True)\n",
    "rbt.gripper.homing()\n",
    "\n",
    "camera_connexion = \"ROS\"\n",
    "if camera_connexion == \"ROS\":\n",
    "    camera = RealCameraROS()\n",
    "    intrinsic, distortion = camera.getIntrinsic()\n",
    "elif camera_connexion == \"pyWrapper\":\n",
    "    camera = RealCamera()\n",
    "    camera.start()\n",
    "    #retrieve image and depth to initialise camera, otherwise image is very dark\n",
    "    for i in range(15):\n",
    "        rgb, depth_image, depth_scale = camera.get_rgb_depth()\n",
    "else:\n",
    "    raise Exception(\"Please choose a valid camera connexion method: ROS or pyWrapper\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PC init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"rbt end effector pos:\", rbt.model.ee_pos_rel())\n",
    "print(\"rbt end effector quat (wxyz):\", rbt.model.ee_orn_rel())\n",
    "print(\"eef rot\",  Rotation.from_quat(convert_quat(rbt.model.ee_orn_rel(), to=\"xyzw\")).as_euler(\"xyz\", degrees=True))\n",
    "print(\"--------------------------------------------------------------------------\")\n",
    "\n",
    "reference_pose = get_camera_pose(rbt)\n",
    "\n",
    "img_init, depth_image, depth_scale = camera.get_rgb_depth()\n",
    "depth_init = depth_image * depth_scale\n",
    "\n",
    "pc = None\n",
    "pc_colors = None\n",
    "pc_init, pc_colors_init = depth2pc(depth_init, intrinsic, img_init)\n",
    "pc2_msg = format_pointcloud_msg(pc_init.copy(), pc_colors_init.copy())\n",
    "bgr_msg = bridge.cv2_to_imgmsg(img_init, encoding=\"bgr8\")\n",
    "\n",
    "init_pos = rbt.model.ee_pos_rel()\n",
    "init_orn_wxyz = rbt.model.ee_orn_rel()\n",
    "\n",
    "# view_init_pos = [0.17908377, 0.23450877, 0.44783125]\n",
    "# view_init_orn_wxyz = [0.0030477, 0.92571715, 0.37800879, 0.01215875]\n",
    "# view_front_pos = [0.36072976, 0.41291873, 0.30648647]\n",
    "# view_front_orn_wxyz = [ 0.1655683,   0.81494402,  0.28065989, -0.47925298]\n",
    "# view_right_pos = [0.10118165, 0.47394513, 0.30265239]\n",
    "# view_right_orn_wxyz = [ 0.3190109,   0.94507174, -0.07051679,  0.00994117]\n",
    "# view_back_pos = [0.00465632, 0.32838561, 0.26232907]\n",
    "# view_back_orn_wxyz = [ 0.06700862,  0.96685827, -0.02494526,  0.24509727]\n",
    "\n",
    "# view_pose = [get_camera_pose(rbt)]\n",
    "# views_pos = [init_pos, view_front_pos, view_right_pos, view_back_pos]\n",
    "# views_orn_wxyz = [init_orn_wxyz, view_front_orn_wxyz, view_right_orn_wxyz, view_back_orn_wxyz]\n",
    "\n",
    "#load pos and orn from json file\n",
    "with open('config/views_pos.json') as json_file:\n",
    "    views_pos = json.load(json_file)\n",
    "\n",
    "keys = views_pos.keys()\n",
    "for key in keys:\n",
    "    print(views_pos[key][\"pos\"])\n",
    "    print(views_pos[key][\"orn_wxyz\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grasping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eef_pos, eef_quat = [], []\n",
    "traj_gen = iLQR(rbt)\n",
    "rbt.stop_controller()\n",
    "rbt.error_recovery()\n",
    "rbt.switch_controller(\"joint_velocity_controller\")\n",
    "rbt.active_controller\n",
    "# Ros CV bridge to convert data from opencv to ROSImage\n",
    "\n",
    "# Call to the server\n",
    "orn, pos, opening, score, detected, detected_with_collision = generate_grasps_client(pc2_msg, bgr_msg)\n",
    "if (detected or detected_with_collision) and opening>0.03:\n",
    "    grasp_pos_world, grasps_orn_world_xyzw = poseCam2World(pos, orn, reference_pose)\n",
    "    grasp_orn_world_wxyz =  convert_quat(grasps_orn_world_xyzw, to=\"wxyz\")\n",
    "\n",
    "    print(\"--------------------------------------------------------------------------\")\n",
    "    print(\"\\n\\ngrasps in world frame :\\n pos :\", grasp_pos_world, \"\\n grasps_orn_world :\", grasps_orn_world_xyzw, \"\\ngrasp world rot\",  Rotation.from_quat(grasps_orn_world_xyzw).as_euler(\"xyz\", degrees=True))\n",
    "    print(\"--------------------------------------------------------------------------\")\n",
    "else:\n",
    "    pc_fused = pc_init\n",
    "    pc_colors_fused = pc_colors_init\n",
    "    pos_dif = 1000\n",
    "    for key in keys:\n",
    "    # while (not detected or not detected_with_collision):\n",
    "        horizon = 30\n",
    "        while pos_dif > 0.01:\n",
    "            view_jpos, view_x_pos, view_U, view_Ks, view_ds, pos_dif, orn_dif = traj_gen.direct_trajectory(rbt.q, rbt.dq, views_pos[key][\"pos\"], views_pos[key][\"orn_wxyz\"], horizon)\n",
    "            horizon *= 2\n",
    "\n",
    "        view_U = np.array(view_U)\n",
    "        success, idx, eef_pos, eef_quat = run_action(rbt, view_U, 20)\n",
    "        rbt.active_controller.send_command(np.zeros(7))\n",
    "        \n",
    "        img_cv, depth_cv, depth_scale = camera.get_rgb_depth()\n",
    "        depth_cv = depth_cv * depth_scale\n",
    "\n",
    "        current_pose = get_camera_pose(rbt)\n",
    "\n",
    "        pc_fused, pc_colors_fused = add_view2pc(pc_fused, pc_colors_fused, reference_pose, current_pose, new_gbr=img_cv, \n",
    "                                                new_depth=depth_cv, cam_intrisic=intrinsic, regularize=True, voxel_size=0.003)\n",
    "        pc2_msg = format_pointcloud_msg(pc_fused, pc_colors_fused)\n",
    "        bgr_msg = bridge.cv2_to_imgmsg(img_init, encoding=\"bgr8\")\n",
    "\n",
    "        orn, pos, opening, score, detected, detected_with_collision = generate_grasps_client(pc2_msg, bgr_msg)\n",
    "\n",
    "        print(\"detected :\", detected)\n",
    "        if (detected or detected_with_collision) and opening > 0.03:\n",
    "            grasp_pos_world, grasps_orn_world_xyzw = poseCam2World(pos, orn, reference_pose)\n",
    "            grasp_orn_world_wxyz = convert_quat(grasps_orn_world_xyzw, to=\"wxyz\")\n",
    "\n",
    "            print(\"--------------------------------------------------------------------------\")\n",
    "            print(\"\\n\\ngrasps in world frame :\\n pos :\", grasp_pos_world, \"\\n grasps_orn_world :\", grasps_orn_world_xyzw, \"\\ngrasp world rot\",  Rotation.from_quat(grasps_orn_world_xyzw).as_euler(\"xyz\", degrees=True))\n",
    "            print(\"------------------------------------ --------------------------------------\")\n",
    "            \n",
    "            break\n",
    "        # break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grasp_horizon = 60\n",
    "grasp_jpos, grasp_x_pos, grasp_U, grasp_Ks, grasp_ds, pos_dif, orn_dif = traj_gen.grasping_trajectory(rbt.q, rbt.dq, grasp_pos_world, grasp_orn_world_wxyz, grasp_horizon)\n",
    "\n",
    "grasp_q = grasp_jpos[-1]\n",
    "grasp_dq = np.zeros_like(grasp_q)\n",
    "dispose_jpos, dispose_x_pos, dispose_U, dispose_Ks, dispose_ds, pos_dif, orn_dif = traj_gen.dispose_trajectory(grasp_q, grasp_dq, grasp_pos_world, grasp_orn_world_wxyz, dispose_pos, dispose_orn_wxyz, 120)\n",
    "iLQR.plot_trajectory(init_pos, grasp_pos_world, grasp_x_pos, dispose_x_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbt.stop_controller()\n",
    "rbt.error_recovery()\n",
    "rbt.switch_controller(\"joint_velocity_controller\")\n",
    "rbt.active_controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbt.gripper.move(width=opening + 0.015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grasp_U = np.array(grasp_U)\n",
    "success, idx, eef_pos, eef_quat = run_action(rbt, grasp_U[:-30], 20)\n",
    "rbt.active_controller.send_command(np.zeros(7))\n",
    "time.sleep(1)\n",
    "success, idx, eef_pos, eef_quat = run_action(rbt, grasp_U[-30:], 20)\n",
    "rbt.active_controller.send_command(np.zeros(7))\n",
    "rbt.gripper.move(width=opening-0.015)\n",
    "\n",
    "# print(\"rbt end effector pos:\", rbt.model.ee_pos_rel())\n",
    "# print(\"rbt end effector quat (wxyz):\", rbt.model.ee_orn_rel())\n",
    "# print(\"eef rot\",  Rotation.from_quat(rbt.model.ee_orn_rel()).as_euler(\"xyz\", degrees=True))\n",
    "# print(\"--------------------------------------------------------------------------\")\n",
    "# print(\"\\n\\ngrasps in world frame :\\n pos :\", grasp_pos_world, \"\\n grasps_orn_world :\", grasps_orn_world_xyzw, \"\\ngrasp world rot\",  Rotation.from_quat(grasps_orn_world_xyzw).as_euler(\"xyz\", degrees=True))\n",
    "# print(\"--------------------------------------------------------------------------\")\n",
    "# print(\"\\n\\ILQR final pose in world frame :\\n pos :\", grasp_x_pos[-1][:3], \"\\n grasps_orn_world wxyz:\", convert_quat(grasp_x_pos[-1][3:], to=\"xyzw\"), \"\\ngrasp world rot\",  Rotation.from_quat(convert_quat(grasp_x_pos[-1][3:], to=\"xyzw\")).as_euler(\"xyz\", degrees=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispose_U = np.array(dispose_U)\n",
    "success, idx, eef_pos, eef_quat = run_action(rbt, dispose_U, 20)\n",
    "rbt.active_controller.send_command(np.zeros(7))\n",
    "rbt.gripper.move(width=0.07)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbt.gripper.homing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_horizon = 60\n",
    "return_jpos, return_x_pos, return_U, return_Ks, return_ds, pos_dif, orn_dif = traj_gen.direct_trajectory(rbt.q, rbt.dq, init_pos, init_orn_wxyz, return_horizon)\n",
    "return_U = np.array(return_U)\n",
    "success, idx, eef_pos, eef_quat = run_action(rbt, return_U, 20)\n",
    "rbt.active_controller.send_command(np.zeros(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbt.stop_controller()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miscellaneous"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get end effector pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"rbt end effector pos:\", rbt.model.ee_pos_rel())\n",
    "print(\"rbt end effector quat (wxyz):\", rbt.model.ee_orn_rel())\n",
    "print(\"eef rot\",  Rotation.from_quat(convert_quat(rbt.model.ee_orn_rel(), to=\"xyzw\")).as_euler(\"xyz\", degrees=True))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show pc with open3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "pc_o3d = o3d.geometry.PointCloud()\n",
    "pc_o3d.points = o3d.utility.Vector3dVector(pc_init)\n",
    "pc_o3d.colors = o3d.utility.Vector3dVector(pc_colors_init/255)\n",
    "coordinate = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.1)\n",
    "coordinate.translate([0,0 , 0.4])\n",
    "o3d.visualization.draw_geometries([coordinate, pc_o3d])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODEL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## moved to transform utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def regularize_pc(pc, pc_colors, method=\"Voxel\", n_points=10000, voxel_size=0.005):\n",
    "#     \"\"\" Regularize point cloud by either voxel downsampling or random sampling\"\"\"\n",
    "#     if method == \"voxel\":\n",
    "#         pcd = o3d.geometry.PointCloud()\n",
    "#         pcd.points = o3d.utility.Vector3dVector(pc)\n",
    "#         pcd.colors = o3d.utility.Vector3dVector(pc_colors)\n",
    "#         pcd = pcd.voxel_down_sample(voxel_size=voxel_size)\n",
    "#         pc = np.asarray(pcd.points)\n",
    "#         pc_colors = np.asarray(pcd.colors)\n",
    "\n",
    "#     elif method == \"random\":\n",
    "#         if pc.shape[0] > n_points:\n",
    "#             idx = np.random.choice(pc.shape[0], n_points, replace=False)\n",
    "#             pc = pc[idx]\n",
    "#             pc_colors = pc_colors[idx]\n",
    "#         else:\n",
    "#             print(\"Warning: pc has less than {} points\".format(n_points))\n",
    "#     else:\n",
    "#         print(\"Error: method {} not implemented, please chose between [voxel, random]\".format(method))\n",
    "\n",
    "#     return pc, pc_colors\n",
    "\n",
    "# def add_view2pc(pc, pc_colors, main_view_pose, camera_pose, new_pc=None, new_pc_colors=None,\n",
    "#                 new_gbr=None, new_depth=None, cam_intrisic=None, regularize=True, voxel_size=0.005):\n",
    "#     \"\"\" Add new view to point cloud and fuse it with the previous one,\n",
    "#         Need either the new point cloud or the new rgb, depth and camera intrinsic to function\n",
    "     \n",
    "#     Args:\n",
    "#         pc (np.array): point cloud of shape (n_points, 3)\n",
    "#         pc_colors (np.array): point cloud colors of shape (n_points, 3)\n",
    "#         main_view_pose (np.array): pose of the pc origin\n",
    "#         camera_pose (np.array): pose of the new view\n",
    "#         new_pc (np.array): point cloud to fused with pc of shape (n_points, 3)\n",
    "#         new_pc_colors (np.array): point cloud colors to fused with pc of shape (n_points, 3)\n",
    "#         new_gbr (np.array): new rgb image of shape (H, W, 3)\n",
    "#         new_depth (np.array): new depth image of shape (H, W)\n",
    "#         cam_intrisic (np.array): camera intrinsic matrix of shape (3, 3)\n",
    "#         regularize (bool): whether to regularize the point cloud\n",
    "#         voxel_size (float): voxel size for regularization\n",
    "\n",
    "#     Returns:\n",
    "#         fused_pc (np.array): fused point cloud of shape (n_points, 3)\n",
    "#         fused_pc_colors (np.array): fused point cloud colors of shape (n_points, 3)\"\"\"\n",
    "#     if new_pc is None:\n",
    "#         if new_depth is None or new_gbr is None or cam_intrisic is None:\n",
    "#             print(\"Error: (new_pc and new_pc_colors) or (new_depth, new_gbr, cam_intrisic) must be provided\")\n",
    "#             return pc\n",
    "\n",
    "#         new_pc, new_pc_colors = depth2pc(new_depth, cam_intrisic, new_gbr)\n",
    "\n",
    "#     new2main = pose_inv(main_view_pose) @ camera_pose\n",
    "#     new_pc_in_mainView = (new2main[:3, :3] @ new_pc.T).T + new2main[:3,3]\n",
    "\n",
    "#     if pc is not None:\n",
    "#         fused_pc = np.vstack((pc, new_pc_in_mainView))\n",
    "#         fused_pc_colors = np.vstack((pc_colors, new_pc_colors))\n",
    "#     else:\n",
    "#         fused_pc = new_pc_in_mainView\n",
    "#         fused_pc_colors = new_pc_colors\n",
    "\n",
    "#     if regularize:\n",
    "#         fused_pc, fused_pc_colors = regularize_pc(fused_pc, fused_pc_colors, method=\"voxel\", voxel_size=voxel_size)\n",
    "\n",
    "#     return fused_pc, fused_pc_colors\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## moved to camera_utils Camera init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# camera.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_img_from_ros(camera_info, depth_info):\n",
    "#     bridge = CvBridge()\n",
    "#     img_data = rospy.wait_for_message(\"/camera/color/image_raw\", Image, timeout=0.5)\n",
    "#     cv_image = bridge.imgmsg_to_cv2(img_data, \"bgr8\")\n",
    "\n",
    "#     if (camera_info.height, camera_info.width) != (depth_info.height, depth_info.width):\n",
    "#             cv_image = cv2.resize(cv_image, dsize=(depth_info.height, depth_info.width), interpolation=cv2.INTER_AREA)\n",
    "#     return cv_image\n",
    "\n",
    "# def get_depth_from_ros():\n",
    "#     bridge = CvBridge()\n",
    "#     depth_data = rospy.wait_for_message(\"/camera/aligned_depth_to_color/image_raw\", Image, timeout=0.5)\n",
    "#     depth_image = bridge.imgmsg_to_cv2(depth_data, \"16UC1\") * 0.001\n",
    "#     return depth_image\n",
    "\n",
    "# def get_pc_from_ros():\n",
    "#     pointcloud_data = rospy.wait_for_message(\"/camera/depth/color/points\", PointCloud2, timeout=0.5)\n",
    "#     pc = list(point_cloud2.read_points(pointcloud_data, field_names=(\"x\", \"y\", \"z\", \"rgb\")))\n",
    "#     pc_xyz = np.array(list(map(lambda x: x[0:3], pc)))\n",
    "#     pc_rgb = list(map(lambda x: x[3], pc))\n",
    "\n",
    "#     # Received data has rgba encripted in one 4 octect word, we format all data in \"raw\" format to unpack them by 1 octet packet\n",
    "#     raw = struct.pack(\"f\"* len(pc_rgb), *pc_rgb)\n",
    "#     rgba = np.array(struct.unpack(\"BBBB\" *len(pc_rgb), raw))\n",
    "\n",
    "#     #reshape to only keep the rgb part\n",
    "#     pc_rgb = np.reshape(rgba, (-1, 4))[:,:3]\n",
    "\n",
    "#     return pc_xyz, pc_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bridge = CvBridge()\n",
    "# rospy.init_node(\"python_node\",anonymous=True)\n",
    "\n",
    "# try: \n",
    "#     camera_info = rospy.wait_for_message(\"/camera/color/camera_info\", CameraInfo, timeout=0.5)\n",
    "#     depth_info = rospy.wait_for_message(\"/camera/aligned_depth_to_color/camera_info\", CameraInfo, timeout=0.5)\n",
    "#     rgb = get_img_from_ros(camera_info, depth_info)\n",
    "#     depth = get_depth_from_ros()\n",
    "#     pc, pc_colors = get_pc_from_ros()\n",
    "#     intrinsic = np.array(camera_info.K).reshape((3,3))\n",
    "#     distortion = np.array(camera_info.D)\n",
    "#     camera_connexion = \"ROS\"\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(\"No rostopic\")\n",
    "#     camera_connexion = \"pyWrapper\"\n",
    "#     # Load camera\n",
    "#     camera = RealCamera()\n",
    "#     camera.start()\n",
    "#     #retrieve image and depth\n",
    "#     for i in range(15):\n",
    "#         rgb, depth_image, depth_scale = camera.get_rgb_depth()\n",
    "\n",
    "#     rgb, depth_image, depth_scale = camera.get_rgb_depth()\n",
    "#     intrinsic, distortion = camera.getIntrinsic()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
